<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unit 2</title>
    <link rel="stylesheet" href="../../../../public/style.css">
    <link rel="icon" href="../../../../public/logo/favicon_io/favicon.ico">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body class="bg-c">
    <div id="mySidepanel" class="sidepanel">
        <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">×</a>
        <a href="../index.html" class="home">back</a>
        <div class="fix-column-links">
            <a href="#" class="link"></a>
            <div class="botbut">
                <a href="" class="link">Next Topic &rarr;</a>
                <a href="" class="link">&larr; Previous Topic</a>
            </div>
        </div>
    </div>
    <div id="navbar" class="grad">
        <div>
            <div class="openbtn" onclick="openNav()">
                <div id="nav-icon1" for="nav-menu1">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </div>
        </div>
        <div>
            <h2>Unit 2</h2>
        </div>
    </div>
    <div class="content-box">
        <div class="wh">
            <h2>Feed Forward Network</h2>
            <p>Feedforward neural networks (FNNs) are a class of artificial neural networks (ANNs) where the information
                moves in one direction—forward—from the input nodes, through the hidden layers, and finally to the
                output nodes. There are no cycles or loops in the network, which is why it's called "feedforward." This
                network structure is one of the simplest and most commonly used models in machine learning and deep
                learning for a variety of tasks such as classification, regression, and pattern recognition.</p>
            <p><strong>Key Components:</strong></p>
            <ol>
                <li>Input Layer: This layer receives the input data. The number of neurons in this layer corresponds to
                    the number of input features in the dataset.</li>
                <li>Hidden Layer(s): These layers are where computations take place using weights and activation
                    functions. A feedforward network can have one or more hidden layers, making it either a shallow or
                    deep neural network. The hidden layers are responsible for extracting and transforming features from
                    the input data.</li>
                <li>Output Layer: The final layer produces the network's output, which could be a class label (in
                    classification tasks) or a continuous value (in regression tasks). The number of neurons in the
                    output layer depends on the task, such as the number of classes for classification.</li>
                <li>Weights and Biases: Weights determine the influence of a particular input on the output, while
                    biases adjust the output along with the weighted sum of inputs, helping the network fit the data
                    better.</li>
                <li>Activation Functions: Activation functions (e.g., sigmoid, ReLU, or tanh) introduce non-linearity
                    into the network, enabling it to learn complex patterns.</li>
            </ol>
            <p><strong>Learning Process:</strong></p>
            <ul>
                <li>The training process of an FNN involves adjusting the weights and biases of the network to minimize
                    the error between the predicted and actual outputs. This is typically done using optimization
                    algorithms such as gradient descent, and the error is measured using a loss function (like mean
                    squared error for regression or cross-entropy for classification).</li>
                <li>Feedforward networks rely on the concept of supervised learning, where they are trained using
                    labeled datasets. During training, the network computes an output, compares it with the actual
                    label, and backpropagates the error through the network to update the weights in such a way that
                    future predictions become more accurate.</li>
            </ul>
            <p><strong>Variants of Feedforward Networks</strong></p>
            <p>There are several extensions of the basic feedforward network model, which enhance its functionality for
                specific tasks. Two popular variants are:</p>
            <ol>
                <li><strong>Back propagation Neural Network (BPN)</strong>: This is an extension of the feedforward
                    network that uses the back propagation algorithm to update the weights during the training process.
                    BPN is one of the most commonly used learning algorithms for feedforward networks, especially for
                    multi-layer perceptrons (MLPs). It minimizes the error by propagating it backward through the
                    network layers.</li>
                <li><strong>Radial Basis Function Network (RBFN)</strong>: RBFN is another variant of the feedforward
                    network, but it uses radial basis functions as activation functions. RBFNs are particularly useful
                    for interpolation problems, classification, and regression tasks. They consist of an input layer, a
                    hidden layer with radial basis function neurons, and an output layer, typically using linear
                    neurons.</li>
            </ol>
            <div class="in">
                <h3>Backpropagation Neural Network (BPN)</h3>
                <ul>
                    <li>It is a standard method for training Artificial Neural Networks (ANNs).</li>
                    <li>
                        BPN is a method of continuously adjusting the weights of the connections in the network to
                        minimize the difference between the actual output and the desired output. This method aims to
                        find the minimum value of the error in the weight space using the delta rule of gradient
                        descent.
                    </li>
                </ul>

                <p><strong>Steps in BPN:</strong></p>
                <ol>
                    <li>Input <strong>x</strong> is introduced to the network through pre-connected paths.</li>
                    <li>Inputs are modeled using randomly assigned weights <strong>w</strong>.</li>
                    <li>Calculate the output of each neuron, propagating from the input layer to the hidden layer, and
                        then to the output layer.</li>
                    <li>Calculate the error at the output layer. The error can be computed as the difference between the
                        actual output and the desired output (i.e., <strong>Error = Actual Output - Desired
                            Output</strong>).</li>
                    <li>
                        The error is then propagated backward, from the output layer to the hidden layer, and then back
                        to the input layer. The weights are adjusted at each layer to reduce the error. This process is
                        repeated iteratively until the error is minimized.
                    </li>
                </ol>
                <p><strong>Confusion?</strong></p>
                <ul>
                    <li>Even though backpropagation involves "going backward," this backward flow occurs only during the
                        training phase. The backpropagation algorithm computes the gradients of the error with respect
                        to each weight in the network by working backward, but this process is purely for adjusting the
                        weights and is not part of the actual inference or data flow during prediction.</li>
                    <li>Thus, during inference or the actual operation of the network (when you're making predictions),
                        the data still flows strictly in the forward direction—from inputs to outputs. This is why
                        networks trained using backpropagation are still considered feedforward networks. The term
                        "feedforward" refers to how information is processed when the network is used for predictions,
                        not how it learns.</li>
                    <li>BPN ko feedforward network ke under isliye include karte hain kyunki backward direction sirf
                        training ke time pe hota hai. Jab hum network ko train karte hain, tab error ko piche ki taraf
                        propagate karke weights adjust karte hain. Lekin jab actual prediction karte hain, yaani jab hum
                        model ko real data dete hain, toh data sirf forward direction mai flow hota hai—input se output
                        tak.
                        <br>
                        Yeh jo term 'feedforward' hai, yeh sirf prediction ke process ke baare mein hai, training ke
                        time pe kya hota hai, usse nahi. Isliye, BPN ko feedforward neural network mana jata hai, kyunki
                        prediction ke waqt data forward hi move karta hai.
                    </li>
                </ul>
                <div class="wh">
                    <p><strong>Example Problem:</strong> Assume that the neurons have a sigmoid activation function,
                        perform a forward pass and a backward pass on the network. Assume that the actual output of y is
                        0.5 and learning rate is 1.</p>
                    <img src="../../images/sc8.jpeg" alt="">
                    <p><strong>Forward Pass</strong>: Compute output for y<sub>3</sub>, y<sub>4</sub> and y<sub>5</sub>.
                    </p>
                    <ul>
                        <li>a<sub>j</sub> = \( \sum_{j} (w_ij * x_i) \)</li>
                        <br>y<sub>j</sub> = f(aj) = <span class="ms">\( f(x) = \frac{1}{1 + e^{-a_j}} \)</span>
                        <li>y<sub>3</sub> = f(a1) = <span class="ms">\( f(a1) = \frac{1}{1 + e^{-a_1}} \)</span>
                            <br>a1 = (w<sub>13</sub> * x<sub>1</sub> ) + (w<sub>23</sub> * x<sub>2</sub>) = 0.755
                            <br>y<sub>3</sub> = f(0.755) = <span class="ms">\( f(a1) = \frac{1}{1 + e^{-0.755}}
                                \)</span> = 0.68

                        </li>
                        <li>y<sub>4</sub> = f(a2) = <span class="ms">\( f(a2) = \frac{1}{1 + e^{-a_2}} \)</span>
                            <br>a2 = (w<sub>14</sub> * x<sub>1</sub> ) + (w<sub>24</sub> * x<sub>2</sub>) = 0.68
                            <br>y<sub>4</sub> = f(0.68) = <span class="ms">\( f(a1) = \frac{1}{1 + e^{-0.68}} \)</span>
                            = 0.6637

                        </li>
                        <li>y<sub>5</sub> = f(a3) = <span class="ms">\( f(a3) = \frac{1}{1 + e^{-a_1}} \)</span>
                            <br>a3 = (w<sub>35</sub> * y<sub>3</sub> ) + (w<sub>45</sub> * y<sub>4</sub>) = 0.801
                            <br>y<sub>5</sub> = f(0.801) = <span class="ms">\( f(a3) = \frac{1}{1 + e^{-0.801}}
                                \)</span> = 0.69

                        </li>
                        <li><strong>Error = y<sub>target</sub> - y<sub>5</sub> = -0.19</strong></li>
                    </ul>
                    <p>To get closure to the desired output we need to update the weight.</p>
                    <p><strong>Each Weight changed by:</strong></p>
                    <ul>
                        <li><span class="ms">&Delta;w<sub>ij</sub> = &eta;&delta;<sub>j</sub>O<sub>i</sub></span>
                            <ul>
                                <li>&delta;<sub>j</sub> = O<sub>j</sub>(1 - O<sub>j</sub>)(t<sub>j</sub> -
                                    O<sub>j</sub>) if <i>j</i> is an output unit</li>
                                <li>&delta;<sub>j</sub> = O<sub>j</sub>(1 - O<sub>j</sub>)\(
                                    \sum_{k}\)&delta;<sub>k</sub>w<sub>kj</sub> if <i>j</i> is a hidden unit</li>
                            </ul>
                        </li>
                        <li>where &eta; is a constant called the learning rate</li>
                        <li>t<sub>j</sub> is the correct output for unit j</li>
                        <li>&delta;<sub>j</sub> is the error measure for unit j</li>
                    </ul>
                    <p><strong>Backward Pass: Compute &delta;<sub>3</sub>, &delta;<sub>4</sub> and
                            &delta;<sub>5</sub></strong></p>
                    <img src="../../images/sc9.jpeg" alt="">
                    <ul>
                        <li>For output unit:
                            <br>&delta;<sub>5</sub> = y<sub>5</sub>(1-y<sub>5</sub>)(y<sub>target</sub> - y<sub>5</sub>)
                            <br> = 0.69*(1-0.69)*(05-0.69) = -0.0406
                        </li>
                        <li>For hidden unit:
                            <br>&delta;<sub>3</sub> = y<sub>3</sub>(1-y<sub>3</sub>)w35*&delta;<sub>5</sub>
                            <br>0.68*(1-0.68)*(0.3*(-0.0406)) = -0.00265
                        </li>
                        <li>For hidden unit:
                            <br>&delta;<sub>4</sub> = y<sub>4</sub>(1-y<sub>4</sub>)w45*&delta;<sub>5</sub>
                            <br>0.6637*(1-0.6637)*(0.9*(-0.0406)) = -0.0082
                        </li>
                    </ul>
                    <p><strong>Compute new weights</strong></p>
                    <p>&Delta;w<sub>ij</sub> = &eta;&Delta;<sub>j</sub>O<sub>i</sub></p>
                    <ul>
                        <li>&Delta;w<sub>13</sub> = &eta;&Delta;<sub>3</sub>x<sub>1</sub> = 1 * (-0.00265) * 0.35 =
                            −0.0009275
                            <br>&Delta;w<sub>13</sub>(new) = &Delta;w<sub>13</sub> + w<sub>13</sub>(old) = −0.0009275 +
                            0.1 = 0.0991
                        </li>
                        <li>&Delta;w<sub>14</sub> = &eta;&Delta;<sub>4</sub>x<sub>1</sub> = 1 * (-0.0082) * 0.35 =
                            -0.00287
                            <br>&Delta;w<sub>14</sub>(new) = &Delta;w<sub>14</sub> + w<sub>14</sub>(old) = -0.00287 +
                            0.4 = 0.3971
                        </li>
                        <li>&Delta;w<sub>23</sub> = &eta;&Delta;<sub>3</sub>x<sub>2</sub> = 1 * (-0.00265) * 0.9 =
                            -0.002385
                            <br>&Delta;w<sub>23</sub>(new) = &Delta;w<sub>23</sub> + w<sub>23</sub>(old) = -0.002385 +
                            0.4 = 0.7976
                        </li>
                        <li>&Delta;w<sub>24</sub> = &eta;&Delta;<sub>4</sub>x<sub>2</sub> = 1 * (-0.0082) * 0.9 =
                            -0.00738
                            <br>&Delta;w<sub>24</sub>(new) = &Delta;w<sub>24</sub> + w<sub>24</sub>(old) = -0.00738 +
                            0.6 = 0.5926
                        </li>
                        <li>&Delta;w<sub>35</sub> = &eta;&Delta;<sub>5</sub>y<sub>3</sub> = 1 * (-0.0406) * 0.68 =
                            -0.0276
                            <br>&Delta;w<sub>35</sub>(new) = &Delta;w<sub>35</sub> + w<sub>35</sub>(old) = -0.0276 + 0.3
                            = 0.2724
                        </li>
                        <li>&Delta;w<sub>45</sub> = &eta;&Delta;<sub>5</sub>y<sub>4</sub> = 1 * (-0.0406) * 0.6637 =
                            -0.0269
                            <br>&Delta;w<sub>45</sub>(new) = &Delta;w<sub>45</sub> + w<sub>45</sub>(old) = -0.0269 + 0.9
                            = 0.8731
                        </li>
                    </ul>
                    <p><strong>Forward Pass: Compute output y<sub>3</sub>, y<sub>4</sub> and y<sub>5</sub>.</strong></p>
                    <img src="../../images/sc10.jpeg" alt="">
                    <ul>
                        <li>y<sub>3</sub> = f(a1) = <span class="ms">\( f(a1) = \frac{1}{1 + e^{-a_1}} \)</span>
                            <br>a1 = (w<sub>13</sub> * x<sub>1</sub> ) + (w<sub>23</sub> * x<sub>2</sub>) = 0.7525
                            <br>y<sub>3</sub> = f(0.7525) = <span class="ms">\( f(a1) = \frac{1}{1 + e^{-0.7525}}
                                \)</span> = 0.6797

                        </li>
                        <li>y<sub>4</sub> = f(a2) = <span class="ms">\( f(a2) = \frac{1}{1 + e^{-a_2}} \)</span>
                            <br>a2 = (w<sub>14</sub> * x<sub>1</sub> ) + (w<sub>24</sub> * x<sub>2</sub>) = 0.6797
                            <br>y<sub>4</sub> = f(0.6797) = <span class="ms">\( f(a1) = \frac{1}{1 + e^{-0.6797}} \)</span>
                            = 0.6620

                        </li>
                        <li>y<sub>5</sub> = f(a3) = <span class="ms">\( f(a3) = \frac{1}{1 + e^{-a_1}} \)</span>
                            <br>a3 = (w<sub>35</sub> * y<sub>3</sub> ) + (w<sub>45</sub> * y<sub>4</sub>) = 0.7631
                            <br>y<sub>5</sub> = f(0.7631) = <span class="ms">\( f(a3) = \frac{1}{1 + e^{-0.7631}}
                                \)</span> = 0.6820 (Network Output)

                        </li>
                        <li><strong>Error = y<sub>target</sub> - y<sub>5</sub> = -0.182</strong></li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    <script src="../../../../public/main.js"></script>
</body>

</html>