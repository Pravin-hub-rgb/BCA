<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unsupervised Learning Algorithms</title>
    <link rel="stylesheet" href="../../../../public/style.css">
    <link rel="icon" href="../../../../public/logo/favicon_io/favicon.ico">
</head>

<body class="bg-c">
    <div id="mySidepanel" class="sidepanel">
        <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">Ã—</a>
        <a href="../index.html" class="home">back</a>
        <div class="fix-column-links">
            <a href="#" class="link"></a>
            <div class="botbut">
                <a href="" class="link">Next Topic &rarr;</a>
                <a href="" class="link">&larr; Previous Topic</a>
            </div>
        </div>
    </div>
    <div id="navbar" class="grad">
        <div>
            <div class="openbtn" onclick="openNav()">
                <div id="nav-icon1" for="nav-menu1">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </div>
        </div>
        <div>
            <h2>Unsupervised Learning Algorithms</h2>
        </div>
    </div>
    <div class="content-box">
        <pre>
            <code>
Unsupervised Learning Algorithms
|
â”œâ”€â”€ 1. Clustering Techniques
â”‚   â”œâ”€â”€ K-Means
â”‚   â”œâ”€â”€ Silhouette Scores (Evaluation)
â”‚   â”œâ”€â”€ Hierarchical Clustering
â”‚   â”œâ”€â”€ Fuzzy C-Means
â”‚   â””â”€â”€ DBScan (Density-Based Clustering)
â”‚
â””â”€â”€ 2. Dimensionality Reduction Techniques
    â”œâ”€â”€ Low Variance Filter
    â”œâ”€â”€ High Correlation Filter
    â”œâ”€â”€ Backward Feature Elimination
    â”œâ”€â”€ Forward Feature Selection
    â”œâ”€â”€ Principal Component Analysis (PCA)
    â””â”€â”€ Projection Methods
            </code>
        </pre>
        <ul>
            <li>
                In Machine Learning, not all problems come with labeled data. Sometimes, we don't know the exact output
                â€” we just have a lot of raw data and want the system to find hidden patterns or structures in it. Thatâ€™s
                where <strong>Unsupervised Learning</strong> comes in! Itâ€™s a branch of ML where the model learns
                directly from the input data without any guidance. Instead of predicting outputs, it focuses on
                exploring the data â€” grouping it (clustering), reducing its complexity (dimensionality reduction), or
                spotting unusual behavior (anomaly detection).
            </li>
            <li>
                In this unit, we dive into two powerful unsupervised approaches: <strong>clustering</strong> and
                <strong>dimensionality reduction</strong>. Letâ€™s start with clustering, where the goal is to group
                similar data points together. Youâ€™ll explore the classic <strong>K-Means algorithm</strong>, evaluate
                clusters using <strong>Silhouette Scores</strong>, and go deeper with techniques like
                <strong>Hierarchical Clustering</strong>, <strong>Fuzzy C-Means</strong> for soft clustering, and
                <strong>DBScan</strong>, which works great for irregularly shaped clusters.
            </li>
            <li>
                Once clustering is covered, we move on to <strong>dimensionality reduction</strong> â€” a must when youâ€™re
                working with huge datasets containing too many features. Youâ€™ll learn practical filtering techniques
                like <strong>Low Variance</strong> and <strong>High Correlation Filters</strong> to eliminate unhelpful
                features, as well as <strong>Forward and Backward Feature Selection</strong> to choose the most
                meaningful ones. Finally, youâ€™ll explore more advanced methods like <strong>Principal Component Analysis
                    (PCA)</strong> and <strong>Projection Methods</strong>, which reduce complexity while keeping the
                essence of your data intact.
            </li>
        </ul>

        <div class="wh">
            <h2>Clustering Techniques</h2>
            <p>
                Clustering is an unsupervised learning technique used to group similar data points into meaningful
                clusters without any prior labels. It helps in discovering hidden patterns or natural groupings in data.
                For example, grouping customers by purchasing behavior, or organizing documents by topic. In clustering,
                the goal is to ensure that data points within the same cluster are more similar to each other than to
                those in other clusters. There are various clustering techniques like K-Means, Hierarchical Clustering,
                and DBSCAN â€” each with its own approach to forming these groups.
            </p>

            <div class="in">
                <h3>K-Means Clustering</h3>
                <p>
                    K-Means is one of the most popular unsupervised learning algorithms used to group similar data
                    points into clusters. It is mainly used in clustering problems where we don't have labeled data. The
                    goal is to divide the data into <strong>K</strong> groups (clusters) based on similarity.
                </p>

                <h4>Key Concepts</h4>
                <ul>
                    <li><strong>Centroid:</strong> The center of a cluster.</li>
                    <li><strong>K:</strong> Number of clusters we want to form.</li>
                    <li><strong>Euclidean Distance:</strong> The distance between a data point and a centroid.</li>
                </ul>

                <h4>How K-Means Works (Incremental Method)</h4>
                <ul>
                    <li>Step 1: Choose the number of clusters K.</li>
                    <li>Step 2: Randomly initialize K centroids.</li>
                    <li>Step 3: Go through each data point one by one:</li>
                    <ul>
                        <li>Assign it to the nearest centroid (based on distance).</li>
                        <li><strong>Immediately update</strong> the centroid of that cluster using the mean of current
                            members.</li>
                    </ul>
                    <li>Step 4: Repeat this for all points. Optionally loop again if needed.</li>
                </ul>

                <h4>Euclidean Distance Formula</h4>
                <p>This formula is used to measure how far two points are from each other in a 2D space (like age and
                    amount):</p>
                <p class="ms">
                    \( d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \)
                </p>

                <h4>Example Dataset</h4>
                <p>Letâ€™s consider the following dataset:</p>
                <div class="table-wrapper">
                    <table class="new-table">
                        <thead>
                            <tr>
                                <th>Client</th>
                                <th>Age</th>
                                <th>Amount</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>C1</td>
                                <td>20</td>
                                <td>500</td>
                            </tr>
                            <tr>
                                <td>C2</td>
                                <td>40</td>
                                <td>1000</td>
                            </tr>
                            <tr>
                                <td>C3</td>
                                <td>30</td>
                                <td>800</td>
                            </tr>
                            <tr>
                                <td>C4</td>
                                <td>18</td>
                                <td>300</td>
                            </tr>
                            <tr>
                                <td>C5</td>
                                <td>28</td>
                                <td>1200</td>
                            </tr>
                            <tr>
                                <td>C6</td>
                                <td>35</td>
                                <td>1400</td>
                            </tr>
                            <tr>
                                <td>C7</td>
                                <td>45</td>
                                <td>1800</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h4>K-Means (K = 2)</h4>
                <ul>
                    <li>Step 1: Choose K = 2 (we want 2 clusters).</li>
                    <li>Step 2: Randomly select 2 initial centroids:</li>
                    <ul>
                        <li>Centroid A = C1 = (20, 500)</li>
                        <li>Centroid B = C2 = (40, 1000)</li>
                    </ul>
                </ul>

                <h4>Step-by-Step Assignment & Updating</h4>

                <ul>
                    <li><strong>Point C3 = (30, 800):</strong></li>
                    <ul>
                        <li>Distance to A = âˆš((30 - 20)Â² + (800 - 500)Â²) = âˆš(100 + 90000) = âˆš90100 â‰ˆ 300.17</li>
                        <li>Distance to B = âˆš((30 - 40)Â² + (800 - 1000)Â²) = âˆš(100 + 40000) = âˆš40100 â‰ˆ 200.25</li>
                        <li>â†’ Assign to Cluster B</li>
                        <li>Update Centroid B = Average of (40,1000) and (30,800) = (35, 900)</li>
                    </ul>

                    <li><strong>Point C4 = (18, 300):</strong></li>
                    <ul>
                        <li>Distance to A = âˆš((18 - 20)Â² + (300 - 500)Â²) = âˆš(4 + 40000) = âˆš40004 â‰ˆ 200.01</li>
                        <li>Distance to B = âˆš((18 - 35)Â² + (300 - 900)Â²) = âˆš(289 + 360000) = âˆš360289 â‰ˆ 600.24</li>
                        <li>â†’ Assign to Cluster A</li>
                        <li>Update Centroid A = Average of (20,500) and (18,300) = (19, 400)</li>
                    </ul>

                    <li><strong>Point C5 = (28, 1200):</strong></li>
                    <ul>
                        <li>Distance to A = âˆš((28 - 19)Â² + (1200 - 400)Â²) = âˆš(81 + 640000) = âˆš640081 â‰ˆ 800.05</li>
                        <li>Distance to B = âˆš((28 - 35)Â² + (1200 - 900)Â²) = âˆš(49 + 90000) = âˆš90049 â‰ˆ 300.08</li>
                        <li>â†’ Assign to Cluster B</li>
                        <li>Update Centroid B = Avg of (40,1000), (30,800), (28,1200) = (32.67, 1000)</li>
                    </ul>

                    <li><strong>Point C6 = (35, 1400):</strong></li>
                    <ul>
                        <li>Distance to A = âˆš((35 - 19)Â² + (1400 - 400)Â²) = âˆš(256 + 1000000) = âˆš1000256 â‰ˆ 1000.13</li>
                        <li>Distance to B = âˆš((35 - 32.67)Â² + (1400 - 1000)Â²) = âˆš(5.45 + 160000) = âˆš160005.45 â‰ˆ 400.01
                        </li>
                        <li>â†’ Assign to Cluster B</li>
                        <li>Update Centroid B = Avg of C2, C3, C5, C6 = (40+30+28+35)/4 = 133/4 = 33.25 and
                            (1000+800+1200+1400)/4 = 4400/4 = 1100</li>
                        <li>â†’ New Centroid B = (33.25, 1100)</li>
                    </ul>

                    <li><strong>Point C7 = (45, 1800):</strong></li>
                    <ul>
                        <li>Distance to A = âˆš((45 - 19)Â² + (1800 - 400)Â²) = âˆš(676 + 1960000) = âˆš1960676 â‰ˆ 1400.24</li>
                        <li>Distance to B = âˆš((45 - 33.25)Â² + (1800 - 1100)Â²) = âˆš(138.06 + 490000) = âˆš490138.06 â‰ˆ 700.10
                        </li>
                        <li>â†’ Assign to Cluster B</li>
                        <li>Update Centroid B = Avg of C2, C3, C5, C6, C7 = (40+30+28+35+45)/5 = 178/5 = 35.6 and
                            (1000+800+1200+1400+1800)/5 = 6200/5 = 1240</li>
                        <li>â†’ Final Centroid B = (35.6, 1240)</li>
                    </ul>
                </ul>

                <h4>Final Clusters (After All Points)</h4>
                <ul>
                    <li>Cluster A (Centroid â‰ˆ 19, 400): C1, C4</li>
                    <li>Cluster B (Centroid â‰ˆ 35.6, 1240): C2, C3, C5, C6, C7</li>
                </ul>
            </div>
            <div class="in">
                <h3>Silhouette Score (Clustering Evaluation)</h3>

                <p>
                    After performing clustering (like K-Means), it's important to check how good the clustering
                    actually is.
                    One of the most popular ways to evaluate clustering quality is using the <strong>Silhouette
                        Score</strong>.
                </p>

                <h4>What is Silhouette Score?</h4>
                <p>
                    Silhouette Score tells us how well a data point fits into its own cluster and how far it is from
                    the other clusters.
                    It gives a score between <strong>-1</strong> and <strong>1</strong> for each point:
                </p>
                <ul>
                    <li><strong>+1:</strong> Perfectly assigned to its cluster.</li>
                    <li><strong>0:</strong> On the boundary between two clusters.</li>
                    <li><strong>Negative:</strong> Possibly assigned to the wrong cluster.</li>
                </ul>

                <h4>Formula</h4>
                <p class="ms">
                    For a single data point <code>i</code>:
                    <br />
                    Let:
                    <br />
                    <code>a(i)</code> = average distance from <code>i</code> to all other points in the <strong>same
                        cluster</strong>
                    <br />
                    <code>b(i)</code> = average distance from <code>i</code> to all points in the <strong>nearest
                        other cluster</strong>
                    <br /><br />
                    Then:
                    <br />
                    <code>S(i) = (b(i) - a(i)) / max(a(i), b(i))</code>
                </p>

                <h4>Steps to Calculate Silhouette Score</h4>
                <ul>
                    <li>Step 1: Perform clustering (e.g., K-Means).</li>
                    <li>Step 2: For each point:
                        <ul>
                            <li>Find <code>a(i)</code> = avg. distance to other points in same cluster.</li>
                            <li>Find <code>b(i)</code> = avg. distance to points in the nearest other cluster.</li>
                            <li>Calculate silhouette score for that point.</li>
                        </ul>
                    </li>
                    <li>Step 3: Take the average of all <code>S(i)</code> to get the overall Silhouette Score.</li>
                </ul>

                <h4>Simple Example</h4>
                <p>Assume we have 4 points clustered into 2 clusters:</p>

                <ul>
                    <li><strong>Cluster A:</strong> P1 = (1, 2), P2 = (2, 2)</li>
                    <li><strong>Cluster B:</strong> P3 = (8, 8), P4 = (9, 9)</li>
                </ul>

                <p>Letâ€™s calculate Silhouette Score for P1:</p>
                <ul>
                    <li><strong>a(P1):</strong> Distance to P2
                        <br />
                        <code>a = âˆš((2-1)Â² + (2-2)Â²) = âˆš(1) = 1.0</code>
                    </li>
                    <li><strong>b(P1):</strong> Avg distance to P3 and P4
                        <br />
                        <code>
                          d1 = âˆš((8-1)Â² + (8-2)Â²) = âˆš(49 + 36) = âˆš85 â‰ˆ 9.22
                          <br />
                          d2 = âˆš((9-1)Â² + (9-2)Â²) = âˆš(64 + 49) = âˆš113 â‰ˆ 10.63
                          <br />
                          b = (9.22 + 10.63) / 2 â‰ˆ 9.93
                        </code>
                    </li>
                    <li>
                        <strong>Silhouette Score for P1:</strong>
                        <br />
                        <code>
                          S = (b - a) / max(a, b) = (9.93 - 1) / 9.93 â‰ˆ 0.899
                        </code>
                    </li>
                </ul>

                <h4>Interpretation</h4>
                <p>
                    Since P1â€™s score is close to 1, it means it's nicely separated and properly clustered.
                    If most points have high scores, your clustering is good. If many have low or negative scores,
                    your clusters may overlap or be wrongly assigned.
                </p>

                <h4>Advantages of Silhouette Score</h4>
                <ul>
                    <li>Easy to understand and visualize.</li>
                    <li>Works without ground truth labels.</li>
                    <li>Can help decide the optimal number of clusters (K).</li>
                </ul>

                <h4>Limitations</h4>
                <ul>
                    <li>Computationally expensive on large datasets.</li>
                    <li>Assumes convex cluster shapes (works best with spherical clusters).</li>
                </ul>

                <h4>Note:</h4>
                <p>
                    You can also use libraries like <code>sklearn.metrics.silhouette_score()</code> in Python to
                    calculate this automatically.
                </p>
            </div>
            <div class="in">
                <h3>Hierarchical Clustering</h3>
              
                <p>
                  Hierarchical clustering builds a hierarchy of clusters. Unlike K-Means, it does not require
                  choosing the number of clusters (K) beforehand. There are two main approaches:
                </p>
              
                <h4>Types of Hierarchical Clustering</h4>
                <ul>
                  <li><strong>Agglomerative (Bottom-Up):</strong> Start with each point as its own cluster, and keep merging the closest pairs until one cluster remains.</li>
                  <li><strong>Divisive (Top-Down):</strong> Start with all points in one cluster, and keep splitting it into smaller clusters.</li>
                </ul>
              
                <h4>Distance Measurement (Linkage Methods)</h4>
                <ul>
                  <li><strong>Single Linkage:</strong> Distance between the closest pair of points in two clusters.</li>
                  <li><strong>Complete Linkage:</strong> Distance between the farthest pair of points in two clusters.</li>
                  <li><strong>Average Linkage:</strong> Average of all pairwise distances between points in two clusters.</li>
                </ul>
              
                <h4>Example: Agglomerative Clustering</h4>
                <p>Given 1D data points: <code>[1, 5, 8, 10, 19, 20]</code></p>
              
                <p>Step-by-step using <strong>single linkage</strong>:</p>
                <ol>
                  <li>Start with each point as its own cluster:
                    <br />
                    [1], [5], [8], [10], [19], [20]
                  </li>
              
                  <li>Find the closest pair (minimum distance):  
                    <br />
                    Distance(19, 20) = 1 â†’ Merge â†’ [19, 20]
                  </li>
              
                  <li>Next closest:  
                    <br />
                    Distance(8, 10) = 2 â†’ Merge â†’ [8, 10]
                  </li>
              
                  <li>Next closest:  
                    <br />
                    Distance(5, 8) = 3 â†’ Merge â†’ [5, 8, 10]
                  </li>
              
                  <li>Next closest:  
                    <br />
                    Distance(1, 5) = 4 â†’ Merge â†’ [1, 5, 8, 10]
                  </li>
              
                  <li>Final merge:
                    <br />
                    Distance(10, 19) = 9 â†’ Merge all â†’ [1, 5, 8, 10, 19, 20]
                  </li>
                </ol>
              <img src="../../images/ml5.svg" alt="" class="wb">
                <h4>Dendrogram</h4>
                <p>
                  A dendrogram is a tree-like diagram that shows the sequence of merges (or splits).
                  You can â€œcutâ€ the dendrogram at any level to get the desired number of clusters.
                </p>
              
                <h4>Divisive Clustering (Top-Down)</h4>
                <p>
                  This method starts with all points in one big cluster and splits them recursively.
                  It's less common and computationally heavier than agglomerative.
                </p>
              
                <p>Steps:</p>
                <ol>
                  <li>Start with all points in one cluster: [1, 5, 8, 10, 19, 20]</li>
                  <li>Split into two clusters based on large distance gap â†’ [1, 5, 8, 10] and [19, 20]</li>
                  <li>Keep splitting each sub-cluster until each point is separate.</li>
                </ol>
              <img src="../../images/ml6.svg" alt="" class="wb">
                <h4>Advantages</h4>
                <ul>
                  <li>No need to pre-define K (number of clusters).</li>
                  <li>Produces a full cluster hierarchy.</li>
                  <li>Works well with small datasets and dendrograms help visualize structure.</li>
                </ul>
              
                <h4>Limitations</h4>
                <ul>
                  <li>Computationally expensive for large datasets.</li>
                  <li>Not suitable for streaming or dynamic data.</li>
                  <li>Once a merge/split happens, it can't be undone.</li>
                </ul>
              
                <h4>Note:</h4>
                <p>
                  Use libraries like <code>scipy.cluster.hierarchy</code> in Python to create dendrograms and perform clustering automatically.
                </p>
              </div>
              
        </div>
    </div>
    <div class="content-box">
        <p>Reference:</p>
        <ul>
            <li><a href="https://www.youtube.com/watch?v=5FpsGnkbEpM" target="_blank">K-mean Clustering with Numerical
                    Example | Unsupervised Learning | MachineğŸ–¥ï¸ Learning ğŸ™‡â€â™‚ï¸ğŸ™‡ Video Lecture &neArr;</a></li>
            <li><a href="https://www.youtube.com/watch?v=zxQF8Rmpk1M" target="_blank"> Hierarchical Clustering |
                    Agglomerative vs Divisive with examples Video Lecture &neArr;</a></li>
        </ul>
    </div>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
    <script type="module" src="../../../../public/main.js"></script>
</body>

</html>