<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unsupervised Learning Algorithms</title>
    <link rel="stylesheet" href="../../../../public/style.css">
    <link rel="icon" href="../../../../public/logo/favicon_io/favicon.ico">
</head>

<body class="bg-c">
    <div id="mySidepanel" class="sidepanel">
        <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">√ó</a>
        <a href="../index.html" class="home">back</a>
        <div class="fix-column-links">
            <a href="#" class="link"></a>
            <div class="botbut">
                <a href="" class="link">Next Topic &rarr;</a>
                <a href="" class="link">&larr; Previous Topic</a>
            </div>
        </div>
    </div>
    <div id="navbar" class="grad">
        <div>
            <div class="openbtn" onclick="openNav()">
                <div id="nav-icon1" for="nav-menu1">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </div>
        </div>
        <div>
            <h2>Unsupervised Learning Algorithms</h2>
        </div>
    </div>
    <div class="content-box">
        <pre>
            <code>
Unsupervised Learning Algorithms
|
‚îú‚îÄ‚îÄ 1. Clustering Techniques
‚îÇ   ‚îú‚îÄ‚îÄ K-Means
‚îÇ   ‚îú‚îÄ‚îÄ Silhouette Scores (Evaluation)
‚îÇ   ‚îú‚îÄ‚îÄ Hierarchical Clustering
‚îÇ   ‚îú‚îÄ‚îÄ Fuzzy C-Means
‚îÇ   ‚îî‚îÄ‚îÄ DBScan (Density-Based Clustering)
‚îÇ
‚îî‚îÄ‚îÄ 2. Dimensionality Reduction Techniques
    ‚îú‚îÄ‚îÄ Low Variance Filter
    ‚îú‚îÄ‚îÄ High Correlation Filter
    ‚îú‚îÄ‚îÄ Backward Feature Elimination
    ‚îú‚îÄ‚îÄ Forward Feature Selection
    ‚îú‚îÄ‚îÄ Principal Component Analysis (PCA)
    ‚îî‚îÄ‚îÄ Projection Methods
            </code>
        </pre>
        <ul>
            <li>
                In Machine Learning, not all problems come with labeled data. Sometimes, we don't know the exact output
                ‚Äî we just have a lot of raw data and want the system to find hidden patterns or structures in it. That‚Äôs
                where <strong>Unsupervised Learning</strong> comes in! It‚Äôs a branch of ML where the model learns
                directly from the input data without any guidance. Instead of predicting outputs, it focuses on
                exploring the data ‚Äî grouping it (clustering), reducing its complexity (dimensionality reduction), or
                spotting unusual behavior (anomaly detection).
            </li>
            <li>
                In this unit, we dive into two powerful unsupervised approaches: <strong>clustering</strong> and
                <strong>dimensionality reduction</strong>. Let‚Äôs start with clustering, where the goal is to group
                similar data points together. You‚Äôll explore the classic <strong>K-Means algorithm</strong>, evaluate
                clusters using <strong>Silhouette Scores</strong>, and go deeper with techniques like
                <strong>Hierarchical Clustering</strong>, <strong>Fuzzy C-Means</strong> for soft clustering, and
                <strong>DBScan</strong>, which works great for irregularly shaped clusters.
            </li>
            <li>
                Once clustering is covered, we move on to <strong>dimensionality reduction</strong> ‚Äî a must when you‚Äôre
                working with huge datasets containing too many features. You‚Äôll learn practical filtering techniques
                like <strong>Low Variance</strong> and <strong>High Correlation Filters</strong> to eliminate unhelpful
                features, as well as <strong>Forward and Backward Feature Selection</strong> to choose the most
                meaningful ones. Finally, you‚Äôll explore more advanced methods like <strong>Principal Component Analysis
                    (PCA)</strong> and <strong>Projection Methods</strong>, which reduce complexity while keeping the
                essence of your data intact.
            </li>
        </ul>

        <div class="wh">
            <h2>Clustering Techniques</h2>
            <p>
                Clustering is an unsupervised learning technique used to group similar data points into meaningful
                clusters without any prior labels. It helps in discovering hidden patterns or natural groupings in data.
                For example, grouping customers by purchasing behavior, or organizing documents by topic. In clustering,
                the goal is to ensure that data points within the same cluster are more similar to each other than to
                those in other clusters. There are various clustering techniques like K-Means, Hierarchical Clustering,
                and DBSCAN ‚Äî each with its own approach to forming these groups.
            </p>

            <div class="in">
                <h3>K-Means Clustering</h3>
                <p>
                    K-Means is one of the most popular unsupervised learning algorithms used to group similar data
                    points into clusters. It is mainly used in clustering problems where we don't have labeled data. The
                    goal is to divide the data into <strong>K</strong> groups (clusters) based on similarity.
                </p>

                <h4>Key Concepts</h4>
                <ul>
                    <li><strong>Centroid:</strong> The center of a cluster.</li>
                    <li><strong>K:</strong> Number of clusters we want to form.</li>
                    <li><strong>Euclidean Distance:</strong> The distance between a data point and a centroid.</li>
                </ul>

                <h4>How K-Means Works (Incremental Method)</h4>
                <ul>
                    <li>Step 1: Choose the number of clusters K.</li>
                    <li>Step 2: Randomly initialize K centroids.</li>
                    <li>Step 3: Go through each data point one by one:</li>
                    <ul>
                        <li>Assign it to the nearest centroid (based on distance).</li>
                        <li><strong>Immediately update</strong> the centroid of that cluster using the mean of current
                            members.</li>
                    </ul>
                    <li>Step 4: Repeat this for all points. Optionally loop again if needed.</li>
                </ul>

                <h4>Euclidean Distance Formula</h4>
                <p>This formula is used to measure how far two points are from each other in a 2D space (like age and
                    amount):</p>
                <p class="ms">
                    \( d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} \)
                </p>

                <h4>Example Dataset</h4>
                <p>Let‚Äôs consider the following dataset:</p>
                <div class="table-wrapper">
                    <table class="new-table">
                        <thead>
                            <tr>
                                <th>Client</th>
                                <th>Age</th>
                                <th>Amount</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>C1</td>
                                <td>20</td>
                                <td>500</td>
                            </tr>
                            <tr>
                                <td>C2</td>
                                <td>40</td>
                                <td>1000</td>
                            </tr>
                            <tr>
                                <td>C3</td>
                                <td>30</td>
                                <td>800</td>
                            </tr>
                            <tr>
                                <td>C4</td>
                                <td>18</td>
                                <td>300</td>
                            </tr>
                            <tr>
                                <td>C5</td>
                                <td>28</td>
                                <td>1200</td>
                            </tr>
                            <tr>
                                <td>C6</td>
                                <td>35</td>
                                <td>1400</td>
                            </tr>
                            <tr>
                                <td>C7</td>
                                <td>45</td>
                                <td>1800</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h4>K-Means (K = 2)</h4>
                <ul>
                    <li>Step 1: Choose K = 2 (we want 2 clusters).</li>
                    <li>Step 2: Randomly select 2 initial centroids:</li>
                    <ul>
                        <li>Centroid A = C1 = (20, 500)</li>
                        <li>Centroid B = C2 = (40, 1000)</li>
                    </ul>
                </ul>

                <h4>Step-by-Step Assignment & Updating</h4>

                <ul>
                    <li><strong>Point C3 = (30, 800):</strong></li>
                    <ul>
                        <li>Distance to A = ‚àö((30 - 20)¬≤ + (800 - 500)¬≤) = ‚àö(100 + 90000) = ‚àö90100 ‚âà 300.17</li>
                        <li>Distance to B = ‚àö((30 - 40)¬≤ + (800 - 1000)¬≤) = ‚àö(100 + 40000) = ‚àö40100 ‚âà 200.25</li>
                        <li>‚Üí Assign to Cluster B</li>
                        <li>Update Centroid B = Average of (40,1000) and (30,800) = (35, 900)</li>
                    </ul>

                    <li><strong>Point C4 = (18, 300):</strong></li>
                    <ul>
                        <li>Distance to A = ‚àö((18 - 20)¬≤ + (300 - 500)¬≤) = ‚àö(4 + 40000) = ‚àö40004 ‚âà 200.01</li>
                        <li>Distance to B = ‚àö((18 - 35)¬≤ + (300 - 900)¬≤) = ‚àö(289 + 360000) = ‚àö360289 ‚âà 600.24</li>
                        <li>‚Üí Assign to Cluster A</li>
                        <li>Update Centroid A = Average of (20,500) and (18,300) = (19, 400)</li>
                    </ul>

                    <li><strong>Point C5 = (28, 1200):</strong></li>
                    <ul>
                        <li>Distance to A = ‚àö((28 - 19)¬≤ + (1200 - 400)¬≤) = ‚àö(81 + 640000) = ‚àö640081 ‚âà 800.05</li>
                        <li>Distance to B = ‚àö((28 - 35)¬≤ + (1200 - 900)¬≤) = ‚àö(49 + 90000) = ‚àö90049 ‚âà 300.08</li>
                        <li>‚Üí Assign to Cluster B</li>
                        <li>Update Centroid B = Avg of (40,1000), (30,800), (28,1200) = (32.67, 1000)</li>
                    </ul>

                    <li><strong>Point C6 = (35, 1400):</strong></li>
                    <ul>
                        <li>Distance to A = ‚àö((35 - 19)¬≤ + (1400 - 400)¬≤) = ‚àö(256 + 1000000) = ‚àö1000256 ‚âà 1000.13</li>
                        <li>Distance to B = ‚àö((35 - 32.67)¬≤ + (1400 - 1000)¬≤) = ‚àö(5.45 + 160000) = ‚àö160005.45 ‚âà 400.01
                        </li>
                        <li>‚Üí Assign to Cluster B</li>
                        <li>Update Centroid B = Avg of C2, C3, C5, C6 = (40+30+28+35)/4 = 133/4 = 33.25 and
                            (1000+800+1200+1400)/4 = 4400/4 = 1100</li>
                        <li>‚Üí New Centroid B = (33.25, 1100)</li>
                    </ul>

                    <li><strong>Point C7 = (45, 1800):</strong></li>
                    <ul>
                        <li>Distance to A = ‚àö((45 - 19)¬≤ + (1800 - 400)¬≤) = ‚àö(676 + 1960000) = ‚àö1960676 ‚âà 1400.24</li>
                        <li>Distance to B = ‚àö((45 - 33.25)¬≤ + (1800 - 1100)¬≤) = ‚àö(138.06 + 490000) = ‚àö490138.06 ‚âà 700.10
                        </li>
                        <li>‚Üí Assign to Cluster B</li>
                        <li>Update Centroid B = Avg of C2, C3, C5, C6, C7 = (40+30+28+35+45)/5 = 178/5 = 35.6 and
                            (1000+800+1200+1400+1800)/5 = 6200/5 = 1240</li>
                        <li>‚Üí Final Centroid B = (35.6, 1240)</li>
                    </ul>
                </ul>

                <h4>Final Clusters (After All Points)</h4>
                <ul>
                    <li>Cluster A (Centroid ‚âà 19, 400): C1, C4</li>
                    <li>Cluster B (Centroid ‚âà 35.6, 1240): C2, C3, C5, C6, C7</li>
                </ul>
            </div>
            <div class="in">
                <h3>Silhouette Score (Clustering Evaluation)</h3>

                <p>
                    After performing clustering (like K-Means), it's important to check how good the clustering
                    actually is.
                    One of the most popular ways to evaluate clustering quality is using the <strong>Silhouette
                        Score</strong>.
                </p>

                <h4>What is Silhouette Score?</h4>
                <p>
                    Silhouette Score tells us how well a data point fits into its own cluster and how far it is from
                    the other clusters.
                    It gives a score between <strong>-1</strong> and <strong>1</strong> for each point:
                </p>
                <ul>
                    <li><strong>+1:</strong> Perfectly assigned to its cluster.</li>
                    <li><strong>0:</strong> On the boundary between two clusters.</li>
                    <li><strong>Negative:</strong> Possibly assigned to the wrong cluster.</li>
                </ul>

                <h4>Formula</h4>
                <p class="ms">
                    For a single data point <code>i</code>:
                    <br />
                    Let:
                    <br />
                    <code>a(i)</code> = average distance from <code>i</code> to all other points in the <strong>same
                        cluster</strong>
                    <br />
                    <code>b(i)</code> = average distance from <code>i</code> to all points in the <strong>nearest
                        other cluster</strong>
                    <br /><br />
                    Then:
                    <br />
                    <code>S(i) = (b(i) - a(i)) / max(a(i), b(i))</code>
                </p>

                <h4>Steps to Calculate Silhouette Score</h4>
                <ul>
                    <li>Step 1: Perform clustering (e.g., K-Means).</li>
                    <li>Step 2: For each point:
                        <ul>
                            <li>Find <code>a(i)</code> = avg. distance to other points in same cluster.</li>
                            <li>Find <code>b(i)</code> = avg. distance to points in the nearest other cluster.</li>
                            <li>Calculate silhouette score for that point.</li>
                        </ul>
                    </li>
                    <li>Step 3: Take the average of all <code>S(i)</code> to get the overall Silhouette Score.</li>
                </ul>

                <h4>Simple Example</h4>
                <p>Assume we have 4 points clustered into 2 clusters:</p>

                <ul>
                    <li><strong>Cluster A:</strong> P1 = (1, 2), P2 = (2, 2)</li>
                    <li><strong>Cluster B:</strong> P3 = (8, 8), P4 = (9, 9)</li>
                </ul>

                <p>Let‚Äôs calculate Silhouette Score for P1:</p>
                <ul>
                    <li><strong>a(P1):</strong> Distance to P2
                        <br />
                        <code>a = ‚àö((2-1)¬≤ + (2-2)¬≤) = ‚àö(1) = 1.0</code>
                    </li>
                    <li><strong>b(P1):</strong> Avg distance to P3 and P4
                        <br />
                        <code>
                          d1 = ‚àö((8-1)¬≤ + (8-2)¬≤) = ‚àö(49 + 36) = ‚àö85 ‚âà 9.22
                          <br />
                          d2 = ‚àö((9-1)¬≤ + (9-2)¬≤) = ‚àö(64 + 49) = ‚àö113 ‚âà 10.63
                          <br />
                          b = (9.22 + 10.63) / 2 ‚âà 9.93
                        </code>
                    </li>
                    <li>
                        <strong>Silhouette Score for P1:</strong>
                        <br />
                        <code>
                          S = (b - a) / max(a, b) = (9.93 - 1) / 9.93 ‚âà 0.899
                        </code>
                    </li>
                </ul>

                <h4>Interpretation</h4>
                <p>
                    Since P1‚Äôs score is close to 1, it means it's nicely separated and properly clustered.
                    If most points have high scores, your clustering is good. If many have low or negative scores,
                    your clusters may overlap or be wrongly assigned.
                </p>

                <h4>Advantages of Silhouette Score</h4>
                <ul>
                    <li>Easy to understand and visualize.</li>
                    <li>Works without ground truth labels.</li>
                    <li>Can help decide the optimal number of clusters (K).</li>
                </ul>

                <h4>Limitations</h4>
                <ul>
                    <li>Computationally expensive on large datasets.</li>
                    <li>Assumes convex cluster shapes (works best with spherical clusters).</li>
                </ul>

                <h4>Note:</h4>
                <p>
                    You can also use libraries like <code>sklearn.metrics.silhouette_score()</code> in Python to
                    calculate this automatically.
                </p>
            </div>
            <div class="in">
                <h3>Hierarchical Clustering</h3>

                <p>
                    Hierarchical clustering builds a hierarchy of clusters. Unlike K-Means, it does not require
                    choosing the number of clusters (K) beforehand. There are two main approaches:
                </p>

                <h4>Types of Hierarchical Clustering</h4>
                <ul>
                    <li><strong>Agglomerative (Bottom-Up):</strong> Start with each point as its own cluster, and keep
                        merging the closest pairs until one cluster remains.</li>
                    <li><strong>Divisive (Top-Down):</strong> Start with all points in one cluster, and keep splitting
                        it into smaller clusters.</li>
                </ul>

                <h4>Distance Measurement (Linkage Methods)</h4>
                <ul>
                    <li><strong>Single Linkage:</strong> Distance between the closest pair of points in two clusters.
                    </li>
                    <li><strong>Complete Linkage:</strong> Distance between the farthest pair of points in two clusters.
                    </li>
                    <li><strong>Average Linkage:</strong> Average of all pairwise distances between points in two
                        clusters.</li>
                </ul>

                <h4>Example: Agglomerative Clustering</h4>
                <p>Given 1D data points: <code>[1, 5, 8, 10, 19, 20]</code></p>

                <p>Step-by-step using <strong>single linkage</strong>:</p>
                <ol>
                    <li>Start with each point as its own cluster:
                        <br />
                        [1], [5], [8], [10], [19], [20]
                    </li>

                    <li>Find the closest pair (minimum distance):
                        <br />
                        Distance(19, 20) = 1 ‚Üí Merge ‚Üí [19, 20]
                    </li>

                    <li>Next closest:
                        <br />
                        Distance(8, 10) = 2 ‚Üí Merge ‚Üí [8, 10]
                    </li>

                    <li>Next closest:
                        <br />
                        Distance(5, 8) = 3 ‚Üí Merge ‚Üí [5, 8, 10]
                    </li>

                    <li>Next closest:
                        <br />
                        Distance(1, 5) = 4 ‚Üí Merge ‚Üí [1, 5, 8, 10]
                    </li>

                    <li>Final merge:
                        <br />
                        Distance(10, 19) = 9 ‚Üí Merge all ‚Üí [1, 5, 8, 10, 19, 20]
                    </li>
                </ol>
                <img src="../../images/ml5.svg" alt="" class="wb">
                <h4>Dendrogram</h4>
                <p>
                    A dendrogram is a tree-like diagram that shows the sequence of merges (or splits).
                    You can ‚Äúcut‚Äù the dendrogram at any level to get the desired number of clusters.
                </p>

                <h4>Divisive Clustering (Top-Down)</h4>
                <p>
                    This method starts with all points in one big cluster and splits them recursively.
                    It's less common and computationally heavier than agglomerative.
                </p>

                <p>Steps:</p>
                <ol>
                    <li>Start with all points in one cluster: [1, 5, 8, 10, 19, 20]</li>
                    <li>Split into two clusters based on large distance gap ‚Üí [1, 5, 8, 10] and [19, 20]</li>
                    <li>Keep splitting each sub-cluster until each point is separate.</li>
                </ol>
                <img src="../../images/ml6.svg" alt="" class="wb">
                <h4>Advantages</h4>
                <ul>
                    <li>No need to pre-define K (number of clusters).</li>
                    <li>Produces a full cluster hierarchy.</li>
                    <li>Works well with small datasets and dendrograms help visualize structure.</li>
                </ul>

                <h4>Limitations</h4>
                <ul>
                    <li>Computationally expensive for large datasets.</li>
                    <li>Not suitable for streaming or dynamic data.</li>
                    <li>Once a merge/split happens, it can't be undone.</li>
                </ul>

                <h4>Note:</h4>
                <p>
                    Use libraries like <code>scipy.cluster.hierarchy</code> in Python to create dendrograms and perform
                    clustering automatically.
                </p>
            </div>

        </div>
        <div class="in">
            <h3>DBSCAN Clustering Algorithm</h3>

            <p>
                DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular clustering algorithm
                that groups together points that are closely packed, marking points that lie alone in low-density
                regions
                as noise. It is especially useful when clusters have irregular shapes.
            </p>

            <h4>Key Concepts</h4>
            <ul>
                <li><strong>Epsilon (Œµ):</strong> The maximum radius of the neighborhood around a point. Think of it as
                    how far you look around a person to find friends.</li>
                <li><strong>MinPts:</strong> Minimum number of points required to form a dense region (cluster). Like
                    minimum people needed in a group to call it a crowd.</li>
                <li><strong>Core Point:</strong> A point that has at least MinPts neighbors within Œµ radius.</li>
                <li><strong>Border Point:</strong> A point that has fewer than MinPts neighbors but lies within the
                    neighborhood of a core point.</li>
                <li><strong>Noise Point:</strong> A point that is neither core nor border (isolated points).</li>
            </ul>

            <h4>Real-Life Analogy: Mall Supervisor</h4>
            <p>
                Imagine you are a supervisor in a mall watching the crowd. You define your neighborhood radius (Œµ) as
                how far you look around a person to see if they are in a crowd. You decide that if a group has at least
                4 people (MinPts), you call it a crowd (cluster). If someone is with enough people nearby, they are a
                <em>core point</em>. If they are near a crowd but don't have enough people themselves, they are a
                <em>border point</em>. People wandering alone far from any crowd are <em>noise</em>.
            </p>

            <h4>Example Dataset</h4>
            <div class="table-wrapper">
                <table class="new-table">
                    <thead>
                        <tr>
                            <th>Point</th>
                            <th>X</th>
                            <th>Y</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>P1</td>
                            <td>4.5</td>
                            <td>8</td>
                        </tr>
                        <tr>
                            <td>P2</td>
                            <td>5</td>
                            <td>7</td>
                        </tr>
                        <tr>
                            <td>P3</td>
                            <td>6</td>
                            <td>6.5</td>
                        </tr>
                        <tr>
                            <td>P4</td>
                            <td>7</td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td>P5</td>
                            <td>9</td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td>P6</td>
                            <td>7</td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td>P7</td>
                            <td>8</td>
                            <td>3.5</td>
                        </tr>
                        <tr>
                            <td>P8</td>
                            <td>9</td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td>P9</td>
                            <td>4</td>
                            <td>4</td>
                        </tr>
                        <tr>
                            <td>P10</td>
                            <td>3</td>
                            <td>7.5</td>
                        </tr>
                        <tr>
                            <td>P11</td>
                            <td>4</td>
                            <td>6</td>
                        </tr>
                        <tr>
                            <td>P12</td>
                            <td>3.5</td>
                            <td>5</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h4>Parameters</h4>
            <ul>
                <li><strong>Epsilon (Œµ):</strong> 1.9 (radius to search neighbors)</li>
                <li><strong>MinPts:</strong> 4 (minimum points to form a cluster)</li>
            </ul>

            <h4>Euclidean Distance Formula</h4>
            <p>To find distance between two points \(P_i = (x_i, y_i)\) and \(P_j = (x_j, y_j)\):</p>
            <p class="ms">
                \( d = \sqrt{(x_j - x_i)^2 + (y_j - y_i)^2} \)
            </p>

            <h4>Distance Table (All Pairwise Distances)</h4>
            <p>Calculated distances between points (rounded to 2 decimals):</p>
            <div class="table-wrapper">
                <table class="new-table">
                    <thead>
                        <tr>
                            <th></th>
                            <th>P1</th>
                            <th>P2</th>
                            <th>P3</th>
                            <th>P4</th>
                            <th>P5</th>
                            <th>P6</th>
                            <th>P7</th>
                            <th>P8</th>
                            <th>P9</th>
                            <th>P10</th>
                            <th>P11</th>
                            <th>P12</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>P1</td>
                            <td>0.00</td>
                            <td>1.12</td>
                            <td>2.12</td>
                            <td>3.91</td>
                            <td>6.02</td>
                            <td>5.59</td>
                            <td>5.70</td>
                            <td>5.41</td>
                            <td>4.03</td>
                            <td>1.58</td>
                            <td>2.06</td>
                            <td>3.16</td>
                        </tr>
                        <tr>
                            <td>P2</td>
                            <td>1.12</td>
                            <td>0.00</td>
                            <td>1.12</td>
                            <td>2.83</td>
                            <td>5.00</td>
                            <td>4.47</td>
                            <td>4.61</td>
                            <td>4.47</td>
                            <td>3.16</td>
                            <td>2.06</td>
                            <td>1.41</td>
                            <td>2.50</td>
                        </tr>
                        <tr>
                            <td>P3</td>
                            <td>2.12</td>
                            <td>1.12</td>
                            <td>0.00</td>
                            <td>1.80</td>
                            <td>3.91</td>
                            <td>3.64</td>
                            <td>3.61</td>
                            <td>3.35</td>
                            <td>3.20</td>
                            <td>3.16</td>
                            <td>2.06</td>
                            <td>2.92</td>
                        </tr>
                        <tr>
                            <td>P4</td>
                            <td>3.91</td>
                            <td>2.83</td>
                            <td>1.80</td>
                            <td>0.00</td>
                            <td>2.24</td>
                            <td>2.00</td>
                            <td>1.80</td>
                            <td>2.00</td>
                            <td>3.16</td>
                            <td>4.72</td>
                            <td>3.16</td>
                            <td>3.50</td>
                        </tr>
                        <tr>
                            <td>P5</td>
                            <td>6.02</td>
                            <td>5.00</td>
                            <td>3.91</td>
                            <td>2.24</td>
                            <td>0.00</td>
                            <td>2.24</td>
                            <td>1.12</td>
                            <td>1.00</td>
                            <td>5.00</td>
                            <td>6.95</td>
                            <td>5.39</td>
                            <td>5.59</td>
                        </tr>
                        <tr>
                            <td>P6</td>
                            <td>5.59</td>
                            <td>4.47</td>
                            <td>3.64</td>
                            <td>2.00</td>
                            <td>2.24</td>
                            <td>0.00</td>
                            <td>1.12</td>
                            <td>2.83</td>
                            <td>3.16</td>
                            <td>6.02</td>
                            <td>4.24</td>
                            <td>4.03</td>
                        </tr>
                        <tr>
                            <td>P7</td>
                            <td>5.70</td>
                            <td>4.61</td>
                            <td>3.61</td>
                            <td>1.80</td>
                            <td>1.12</td>
                            <td>1.12</td>
                            <td>0.00</td>
                            <td>1.80</td>
                            <td>4.03</td>
                            <td>6.40</td>
                            <td>4.72</td>
                            <td>4.74</td>
                        </tr>
                        <tr>
                            <td>P8</td>
                            <td>5.41</td>
                            <td>4.47</td>
                            <td>3.35</td>
                            <td>2.00</td>
                            <td>1.00</td>
                            <td>2.83</td>
                            <td>1.80</td>
                            <td>0.00</td>
                            <td>5.10</td>
                            <td>6.50</td>
                            <td>5.10</td>
                            <td>5.50</td>
                        </tr>
                        <tr>
                            <td>P9</td>
                            <td>4.03</td>
                            <td>3.16</td>
                            <td>3.20</td>
                            <td>3.16</td>
                            <td>5.00</td>
                            <td>3.16</td>
                            <td>4.03</td>
                            <td>5.10</td>
                            <td>0.00</td>
                            <td>3.64</td>
                            <td>2.00</td>
                            <td>1.12</td>
                        </tr>
                        <tr>
                            <td>P10</td>
                            <td>1.58</td>
                            <td>2.06</td>
                            <td>3.16</td>
                            <td>4.72</td>
                            <td>6.95</td>
                            <td>6.02</td>
                            <td>6.40</td>
                            <td>6.50</td>
                            <td>3.64</td>
                            <td>0.00</td>
                            <td>1.80</td>
                            <td>2.55</td>
                        </tr>
                        <tr>
                            <td>P11</td>
                            <td>2.06</td>
                            <td>1.41</td>
                            <td>2.06</td>
                            <td>3.16</td>
                            <td>5.39</td>
                            <td>4.24</td>
                            <td>4.72</td>
                            <td>5.10</td>
                            <td>2.00</td>
                            <td>1.80</td>
                            <td>0.00</td>
                            <td>1.12</td>
                        </tr>
                        <tr>
                            <td>P12</td>
                            <td>3.16</td>
                            <td>2.50</td>
                            <td>2.92</td>
                            <td>3.50</td>
                            <td>5.59</td>
                            <td>4.03</td>
                            <td>4.74</td>
                            <td>5.50</td>
                            <td>1.12</td>
                            <td>2.55</td>
                            <td>1.12</td>
                            <td>0.00</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h4>Step 1: Identify Neighbors of Each Point (Within Œµ = 1.9)</h4>
            <p>Check which points are within distance 1.9 for each point:</p>
            <ul>
                <li><strong>P1:</strong> P2 (1.12), P10 (1.58) ‚Üí 2 neighbors + P1 = 3 total</li>
                <li><strong>P2:</strong> P1 (1.12), P3 (1.12), P11 (1.41) ‚Üí 3 neighbors + P2 = 4 total</li>
                <li><strong>P3:</strong> P2 (1.12), P4 (1.80) ‚Üí 2 neighbors + P3 = 3 total</li>
                <li><strong>P4:</strong> P3 (1.80), P7 (1.80) ‚Üí 2 neighbors + P4 = 3 total</li>
                <li><strong>P5:</strong> P7 (1.12), P8 (1.00) ‚Üí 2 neighbors + P5 = 3 total</li>
                <li><strong>P6:</strong> P7 (1.12) ‚Üí 1 neighbor + P6 = 2 total</li>
                <li><strong>P7:</strong> P4 (1.80), P5 (1.12), P6 (1.12), P8 (1.80) ‚Üí 4 neighbors + P7 = 5 total</li>
                <li><strong>P8:</strong> P5 (1.00), P7 (1.80) ‚Üí 2 neighbors + P8 = 3 total</li>
                <li><strong>P9:</strong> P12 (1.12) ‚Üí 1 neighbor + P9 = 2 total</li>
                <li><strong>P10:</strong> P1 (1.58), P11 (1.80) ‚Üí 2 neighbors + P10 = 3 total</li>
                <li><strong>P11:</strong> P2 (1.41), P10 (1.80), P12 (1.12) ‚Üí 3 neighbors + P11 = 4 total</li>
                <li><strong>P12:</strong> P9 (1.12), P11 (1.12) ‚Üí 2 neighbors + P12 = 3 total</li>
            </ul>

            <h4>Step 2: Identify Core, Border, and Noise Points</h4>
            <ul>
                <li><strong>Core Points (‚â• MinPts = 4 total points including itself):</strong>
                    <ul>
                        <li><strong>P2:</strong> Has 4 total points (P1, P3, P11, and itself) ‚Üí Core</li>
                        <li><strong>P7:</strong> Has 5 total points (P4, P5, P6, P8, and itself) ‚Üí Core</li>
                        <li><strong>P11:</strong> Has 4 total points (P2, P10, P12, and itself) ‚Üí Core</li>
                    </ul>
                </li>
                <li><strong>Border Points (less than 4 total points but within Œµ of a core point):</strong>
                    <ul>
                        <li><strong>P1:</strong> Connected to core point P2</li>
                        <li><strong>P3:</strong> Connected to core point P2</li>
                        <li><strong>P4:</strong> Connected to core point P7</li>
                        <li><strong>P5:</strong> Connected to core point P7</li>
                        <li><strong>P6:</strong> Connected to core point P7</li>
                        <li><strong>P8:</strong> Connected to core point P7</li>
                        <li><strong>P10:</strong> Connected to core point P11</li>
                        <li><strong>P12:</strong> Connected to core point P11</li>
                    </ul>
                </li>
                <li><strong>Noise Points (not core and not connected to any core point):</strong>
                    <ul>
                        <li><strong>P9:</strong> Only connected to P12, but not within Œµ of any core point</li>
                    </ul>
                </li>
            </ul>

            <h4>Step 3: Form Clusters</h4>
            <p>Since core points P2 and P11 are connected through their mutual neighborhood (distance 1.41 < 1.9), they form one large cluster:</p>
            <ul>
                <li><strong>Cluster 1:</strong> P1, P2, P3, P10, P11, P12 (6 points)
                    <ul>
                        <li>Core points: P2, P11</li>
                        <li>Border points: P1, P3, P10, P12</li>
                    </ul>
                </li>
                <li><strong>Cluster 2:</strong> P4, P5, P6, P7, P8 (5 points)
                    <ul>
                        <li>Core point: P7</li>
                        <li>Border points: P4, P5, P6, P8</li>
                    </ul>
                </li>
            </ul>
            <img src="../../images/ml7.svg" alt="">
            <h4>Summary</h4>
            <ul>
                <li><strong>Cluster 1:</strong> P1, P2, P3, P10, P11, P12 (6 points)</li>
                <li><strong>Cluster 2:</strong> P4, P5, P6, P7, P8 (5 points)</li>
                <li><strong>Noise Points:</strong> P9 (1 point)</li>
            </ul>

            <h4>Why DBSCAN is Useful?</h4>
            <p>
                - It can find clusters of arbitrary shapes.<br>
                - It automatically identifies noise points.<br>
                - It does not require specifying the number of clusters upfront.<br>
            </p>
        </div>

    </div>
    <div class="content-box">
        <p>Reference:</p>
        <ul>
            <li><a href="https://www.youtube.com/watch?v=5FpsGnkbEpM" target="_blank">K-mean Clustering with Numerical
                    Example | Unsupervised Learning | Machineüñ•Ô∏è Learning üôá‚Äç‚ôÇÔ∏èüôá Video Lecture &neArr;</a></li>
            <li><a href="https://www.youtube.com/watch?v=zxQF8Rmpk1M" target="_blank"> Hierarchical Clustering |
                    Agglomerative vs Divisive with examples Video Lecture &neArr;</a></li>
            <li><a href="https://www.youtube.com/watch?v=gFS_zmgvW_c" target="_blank"> DBSCAN Clustering Algorithm with
                    Numerical example &neArr;</a></li>
        </ul>
    </div>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>
    <script type="module" src="../../../../public/main.js"></script>
</body>

</html>